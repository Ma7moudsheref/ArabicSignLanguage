import streamlit as st
import cv2
import pickle
import mediapipe as mp
import numpy as np
import requests
import os
from PIL import ImageFont, ImageDraw, Image
import arabic_reshaper
from bidi.algorithm import get_display

st.set_page_config(page_title="Ù…ØªØ±Ø¬Ù… Ù„ØºØ© Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠ", layout="centered")

# --- Ø¯Ø§Ù„Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù…Ù† Ø±Ø§Ø¨Ø· MediaFire Ø§Ù„Ù…Ø¨Ø§Ø´Ø± ---
@st.cache_resource
def download_and_load_model():
    # Ù‡Ø°Ø§ Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ù„Ù„Ù…Ù„Ù Ø§Ù„Ù„ÙŠ Ø£Ù†Øª Ø±ÙØ¹ØªÙ‡
    file_url = "https://www.mediafire.com/file/slwpbp2cqiw9gp8/arabic_model.p/file"
    model_path = "arabic_model.p"
    
    if not os.path.exists(model_path):
        with st.spinner('Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù„Ø£ÙˆÙ„ Ù…Ø±Ø©ØŒ Ø§Ù†ØªØ¸Ø± Ù„Ø­Ø¸Ø©...'):
            # Ù…Ù„Ø§Ø­Ø¸Ø©: Ù…ÙŠØ¯ÙŠØ§ ÙØ§ÙŠØ± ÙŠØ­ØªØ§Ø¬ Ø£Ø­ÙŠØ§Ù†Ø§Ù‹ Ø¶ØºØ·Ø© ÙŠØ¯ÙˆÙŠØ©ØŒ Ù„ÙƒÙ† Ø³Ù†Ø­Ø§ÙˆÙ„ ØªØ­Ù…ÙŠÙ„Ù‡ Ø¨Ø±Ù…Ø¬ÙŠØ§Ù‹
            # Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØŒ Ø³Ù†Ø·Ù„Ø¨ Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªØ£ÙƒØ¯
            r = requests.get(file_url, allow_redirects=True)
            with open(model_path, 'wb') as f:
                f.write(r.content)
    
    with open(model_path, 'rb') as f:
        return pickle.load(f)

# Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
try:
    data = download_and_load_model()
    model = data['model']
    label_encoder = data['label_encoder']
    st.success("âœ… ØªÙ… Ø§ØªØµØ§Ù„ Ø§Ù„Ø³ÙŠØ±ÙØ± Ø¨Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­!")
except Exception as e:
    st.error("âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹. ØªØ£ÙƒØ¯ Ù…Ù† Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù arabic_model.p Ø¨Ø¬Ø§Ù†Ø¨ Ø§Ù„ÙƒÙˆØ¯ Ø£Ùˆ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø±Ø§Ø¨Ø·.")

st.title("Ù…ØªØ±Ø¬Ù… Ù„ØºØ© Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠ ğŸ–ï¸")

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ ---
img_file_buffer = st.camera_input("Ø§Ù„ØªÙ‚Ø· ØµÙˆØ±Ø© Ù„Ø¥Ø´Ø§Ø±Ø© ÙŠØ¯Ùƒ Ù„Ù„ØªØ±Ø¬Ù…Ø©")

if img_file_buffer is not None:
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ OpenCV
    bytes_data = img_file_buffer.getvalue()
    cv2_img = cv2.imdecode(np.frombuffer(bytes_data, np.uint8), cv2.IMREAD_COLOR)
    
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ù„Ù€ MediaPipe
    img_rgb = cv2.cvtColor(cv2_img, cv2.COLOR_BGR2RGB)
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ MediaPipe
    mp_hands = mp.solutions.hands
    hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.5)
    results = hands.process(img_rgb)
    
    if results.multi_hand_landmarks:
        for hand_landmarks in results.multi_hand_landmarks:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Ù‚Ø§Ø· (Ù†ÙØ³ Ù…Ù†Ø·Ù‚ Ø¨Ø§ÙŠØªØ´Ø§Ø±Ù…)
            landmarks = np.array([[l.x, l.y, l.z] for l in hand_landmarks.landmark])
            landmarks = landmarks - landmarks[0]
            max_v = np.abs(landmarks).max()
            if max_v > 0: landmarks /= max_v
            
            distances = np.linalg.norm(landmarks, axis=1)
            angle = np.arctan2(landmarks[8][1], landmarks[8][0])
            data_in = np.hstack([landmarks.flatten(), distances, [angle]])
            
            # Ø§Ù„ØªÙˆÙ‚Ø¹
            prediction = model.predict([data_in])[0]
            char = label_encoder.inverse_transform([prediction])[0]
            
            # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨Ø´ÙƒÙ„ Ø´ÙŠÙƒ
            st.balloons()
            st.markdown(f"<h1 style='text-align: center; color: #00ffcc;'>Ø§Ù„Ø­Ø±Ù Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: {char}</h1>", unsafe_allow_html=True)
    else:
        st.warning("Ù„Ù… ÙŠØªÙ… Ø±ØµØ¯ ÙŠØ¯ ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©ØŒ Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.")
